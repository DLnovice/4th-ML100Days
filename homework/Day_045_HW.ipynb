{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [梯度提升決策樹GBDT（Gradient Boosting Decision Tree）]\n",
    "G-B-D-T梯度提升決策樹，顧名思義，是一個與梯度有關、對決策樹進行了提升的機器學習模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- G（Gradient） 梯度\n",
    "    - 梯度的大小反映了當前預測值與目標值之間的距離。因此，下面B所述的序列決策樹模型，除開第一棵決策樹使用原始預測指標建樹，之後的每一棵決策樹都用前一棵決策樹的預測值與目標值計算出來的負梯度（可以理解為殘差或者增量）來建樹。這相當於給分錯的樣本加權多次分類，使樣本最終的殘差趨近於0。除開第一棵樹的其他樹，由於都是對目標的殘差或增量進行建模預測，因此GBDT模型只需把過程中每一棵決策樹的輸出結果累加，便可得到最終的預測輸出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- B（Boosting） 提升\n",
    "    - 即在原來模型的基礎之上做進一步提升，提升決策樹BDT的基本思想是採用多棵決策樹序列建模。具體過程為，對於第一棵樹之後的每一棵決策樹，都基於前一棵決策樹的輸出進行二次建模，整個序列建模過程相當於對預測結果朝目標值進行修正。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DT（Decision Tree） 決策樹\n",
    "    - T自不必多說，作為一種常見的資料結構出現在各種演算法當中。DT決策樹，有分類樹與迴歸樹兩種。迴歸樹原理機制與分類樹相似，區別在於分類樹只有在葉子結點返回唯一分類，而回歸樹的每個節點都能返回預測值，通常為當前節點下所有樣本的均值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 綜上，一句話概括GBDT的核心思想就是：序列訓練n(n > 2)棵決策樹，其中第i(1 < i ≤ n)棵樹學習第i - 1棵樹的負梯度（可理解為殘差或增量），n棵樹的輸出結果累加作為最終輸出結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
